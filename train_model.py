"""
Treina um modelo de regress√£o linear para prever o consumo de combust√≠vel. üöóüí®‚õΩ

Este script treina um modelo de regress√£o linear usando o dataset normalizado e avalia seu desempenho usando o Mean Squared Error (MSE). O modelo treinado √© ent√£o salvo para uso posterior.

‚öôÔ∏è Etapas do Treinamento: ‚öôÔ∏è

1. **Carregamento de Dados:** L√™ o dataset normalizado (`normalized_dataset.csv`).
2. **Prepara√ß√£o de Dados:** Separa as features (X) do target (y) e aplica One-Hot Encoding √† vari√°vel categ√≥rica `vehicle_type`.
3. **Divis√£o de Dados:** Divide os dados em conjuntos de treino e teste (80% treino, 20% teste).
4. **Treinamento do Modelo:** Treina um modelo de `LinearRegression` usando os dados de treino.
5. **Avalia√ß√£o do Modelo:** Avalia o modelo usando o MSE nos dados de teste.
6. **Salvamento do Modelo:** Salva o modelo treinado em um arquivo (`model.joblib`).

üìä M√©tricas de Avalia√ß√£o: üìä

* **Mean Squared Error (MSE):** Mede a m√©dia dos quadrados das diferen√ßas entre os valores previstos e os valores reais. Um MSE menor indica um melhor desempenho do modelo.

üì¶ Entrada e Sa√≠da: üì¶

* **Entrada:** `normalized_dataset.csv`
* **Sa√≠da:** `model.joblib`

"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import joblib
import sys

try:
    df = pd.read_csv('dataset.csv')
except FileNotFoundError:
    error_message = "Erro: O arquivo 'dataset.csv' n√£o foi encontrado. ‚ùå"
    sys.stdout.buffer.write(error_message.encode('utf-8') + b'\n')
    exit()

# Separar features e target
X = df[['distance', 'speed', 'vehicle_type']]
y = df['consumption']

# One-hot encoding para vehicle_type
X = pd.get_dummies(X, columns=['vehicle_type'], prefix=['vehicle_type'])

# Split dos dados
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalizar features num√©ricas (apenas distance e speed)
scaler = StandardScaler()
numeric_cols = ['distance', 'speed']
X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])
X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])

# Treinar modelo
model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42
)

model.fit(X_train, y_train)

# Avaliar modelo
y_pred = model.predict(X_test)
print(f"R¬≤ Score: {r2_score(y_test, y_pred)}")
print(f"MSE: {mean_squared_error(y_test, y_pred)}")

# Verificar import√¢ncia das features
feature_importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': model.feature_importances_
})
print("\nImport√¢ncia das Features:")
print(feature_importance.sort_values('importance', ascending=False))

# Fazer algumas predi√ß√µes de teste
test_data = pd.DataFrame({
    'distance': [1000, 1000, 1000],
    'speed': [90, 85, 75],
    'vehicle_type': ['carro', 'moto', 'caminh√£o']
})

test_data = pd.get_dummies(test_data, columns=['vehicle_type'], prefix=['vehicle_type'])
test_data[numeric_cols] = scaler.transform(test_data[numeric_cols])

predictions = model.predict(test_data)
print("\nPredi√ß√µes de teste (1000km):")
print(f"Carro: {predictions[0]:.2f}L")
print(f"Moto: {predictions[1]:.2f}L")
print(f"Caminh√£o: {predictions[2]:.2f}L")

# Salvar modelo
joblib.dump(model, 'model.joblib')
success_message = "Modelo treinado e salvo com sucesso! ‚úÖ"
sys.stdout.buffer.write(success_message.encode('utf-8') + b'\n')

# Verificar o conte√∫do do dataset
print(df.groupby('vehicle_type')['consumption'].describe())
