# Predi√ß√£o de Consumo de Combust√≠vel - Por Elias Andrade - v0.003

Este projeto utiliza aprendizado de m√°quina para prever o consumo de combust√≠vel.  Sou Elias Andrade, e desenvolvi este projeto como uma demonstra√ß√£o de minhas habilidades em IA e Machine Learning.

## Contato

üè† Localiza√ß√£o: Maring√°, Paran√°, Brasil
üìû Telefone: +55 (44) 98765-4321
üìß E-mail: elias.andrade@email.com

## Descri√ß√£o do Projeto

Este projeto visa prever o consumo de combust√≠vel de ve√≠culos com base em diferentes par√¢metros, utilizando t√©cnicas de aprendizado de m√°quina.  O modelo foi treinado com um dataset sint√©tico, gerado e normalizado para otimizar o desempenho do algoritmo.  A vers√£o atual (v0.003) inclui melhorias na documenta√ß√£o, na clareza do c√≥digo e instru√ß√µes para configurar a√ß√µes do GitHub, Docker, Terraform e Kubernetes.

## Arquitetura do Modelo

O modelo utilizado √© uma Regress√£o Linear, escolhida por sua simplicidade e efic√°cia para este problema espec√≠fico.  A escolha da Regress√£o Linear se justifica pela natureza linear esperada entre as vari√°veis de entrada (dist√¢ncia, velocidade e tipo de ve√≠culo) e a vari√°vel de sa√≠da (consumo de combust√≠vel).  Entretanto, modelos mais complexos poderiam ser explorados para melhorar a precis√£o das previs√µes.  O modelo √© treinado utilizando 80% dos dados, e os 20% restantes s√£o usados para avalia√ß√£o do modelo.

### Dados de Treinamento

O modelo foi treinado utilizando um dataset sint√©tico gerado pelo script `generate_dataset.py`. Este dataset cont√©m as seguintes features:

* **distance:** Dist√¢ncia percorrida (km) üìè - Num√©rica, gerada aleatoriamente entre 10 e 1000 km.
* **speed:** Velocidade m√©dia (km/h) üí® - Num√©rica, gerada aleatoriamente entre 30 e 120 km/h.
* **vehicle_type:** Tipo de ve√≠culo (carro üöó, moto üèçÔ∏è, caminh√£o üöö) - Categ√≥rica, escolhida aleatoriamente entre as tr√™s op√ß√µes.
* **consumption:** Consumo de combust√≠vel (litros) ‚õΩ - Num√©rica, gerada aleatoriamente entre 5 e 50 litros (para simplifica√ß√£o).  Este valor √© um placeholder e pode ser refinado com um modelo mais complexo ou com dados reais.

O dataset original (`dataset.csv`) foi normalizado usando `MinMaxScaler` do scikit-learn, resultando no arquivo `normalized_dataset.csv`.  A normaliza√ß√£o foi aplicada √†s colunas 'distance', 'speed' e 'consumption'.  A normaliza√ß√£o √© crucial para garantir que todas as features contribuam igualmente para o treinamento do modelo, evitando que features com valores maiores dominem o processo de aprendizado.  O `MinMaxScaler` transforma os valores num√©ricos para um intervalo entre 0 e 1.

### Pr√©-processamento dos Dados

O pr√©-processamento dos dados incluiu as seguintes etapas:

1. **Gera√ß√£o de Dados:** Um dataset sint√©tico foi gerado usando o script `generate_dataset.py`. Este script gera dados aleat√≥rios simulando diferentes cen√°rios de condu√ß√£o.  O n√∫mero de amostras pode ser ajustado alterando a vari√°vel `num_samples` no script.
2. **Normaliza√ß√£o:** Os dados num√©ricos foram normalizados usando o `MinMaxScaler` para garantir que todas as features estejam na mesma escala.
3. **One-Hot Encoding:** A vari√°vel categ√≥rica `vehicle_type` foi convertida em representa√ß√£o num√©rica usando One-Hot Encoding.  Esta t√©cnica transforma vari√°veis categ√≥ricas em m√∫ltiplas vari√°veis bin√°rias, permitindo que o modelo as utilize no processo de treinamento.  O script `train_model.py` utiliza `pd.get_dummies` para realizar este processo.

### Treinamento do Modelo

O modelo foi treinado usando o script `train_model.py`. Este script utiliza o `train_test_split` para dividir o dataset em conjuntos de treinamento e teste (80% treino, 20% teste), permitindo avaliar o desempenho do modelo em dados n√£o vistos durante o treinamento.  O modelo foi avaliado usando o Mean Squared Error (MSE), uma m√©trica comum para avaliar modelos de regress√£o.  O MSE mede a m√©dia dos quadrados das diferen√ßas entre os valores previstos e os valores reais. Um MSE menor indica um melhor desempenho do modelo. O modelo treinado √© salvo no arquivo `model.joblib` usando a biblioteca `joblib`.

### Previs√£o do Consumo

O script `predict_consumption.py` utiliza o modelo treinado para gerar previs√µes de consumo de combust√≠vel.  Este script demonstra como carregar o modelo treinado e fazer previs√µes com base em novos dados de entrada.  A sa√≠da do script inclui uma tabela formatada mostrando as previs√µes e a economia em rela√ß√£o √† m√©dia do consumo normalizado.  O script gera dados aleat√≥rios de entrada (dist√¢ncia, velocidade, tipo de ve√≠culo) e utiliza o modelo carregado para prever o consumo.  A biblioteca `rich` √© usada para apresentar os resultados em uma tabela formatada.  O script tamb√©m inclui tratamento de erros para lidar com arquivos n√£o encontrados e outras exce√ß√µes.

## Tecnologias Utilizadas

* **Python:** Linguagem de programa√ß√£o principal.
* **Pandas:** Manipula√ß√£o e an√°lise de dados.
* **Scikit-learn:** Biblioteca para aprendizado de m√°quina (Regress√£o Linear, MinMaxScaler, train_test_split, mean_squared_error).
* **Joblib:** Salvamento e carregamento de modelos.
* **Rich:** Biblioteca para interface de usu√°rio em terminal.
* **NumPy:** Computa√ß√£o num√©rica.

## Pr√≥ximos Passos

* **Aprimoramento do Modelo:** Explorar modelos de aprendizado de m√°quina mais complexos para melhorar a precis√£o das previs√µes.  Considerar modelos como RandomForestRegressor ou GradientBoostingRegressor.
* **Dados Reais:** Utilizar um dataset de dados reais para treinar e avaliar o modelo.  Isso permitir√° uma avalia√ß√£o mais precisa do desempenho do modelo em cen√°rios do mundo real.
* **Interface Gr√°fica:** Desenvolver uma interface gr√°fica para facilitar o uso do modelo.  Uma interface gr√°fica tornaria o modelo mais acess√≠vel a usu√°rios sem experi√™ncia em programa√ß√£o.
* **Integra√ß√£o com Sistemas:** Integrar o modelo com outros sistemas para automatizar o processo de previs√£o.  Isso poderia envolver a integra√ß√£o com sistemas de gerenciamento de frotas ou sistemas de monitoramento de ve√≠culos.
* **Deploy:** Implementar o modelo em um ambiente de produ√ß√£o.  Isso permitiria que o modelo fosse usado em larga escala para prever o consumo de combust√≠vel.
* **GitHub Actions:** Criar uma a√ß√£o para criar backups versionados dos commits.
* **Docker:** Criar um Dockerfile para containerizar a aplica√ß√£o.
* **Terraform:** Criar arquivos Terraform para provisionar infraestrutura.
* **Kubernetes:** Criar configura√ß√µes Kubernetes para implantar a aplica√ß√£o.


## Arquivos

* `generate_dataset.py`: Gera o dataset de treinamento.
* `normalize_data.py`: Normaliza os dados.
* `train_model.py`: Treina o modelo de aprendizado de m√°quina.
* `predict_consumption.py`: Prediz o consumo de combust√≠vel.
* `model.joblib`: Modelo treinado.
* `dataset.csv`: Dataset original.
* `normalized_dataset.csv`: Dataset normalizado.
* `README.md`: Este arquivo.
* `combined_documentation.md`: Documenta√ß√£o combinada.
* `model_architecture.md`: Arquitetura do modelo.


## Como Executar

1. **Instalar depend√™ncias:** `pip install pandas scikit-learn joblib rich numpy`
2. **Gerar dataset:** `python generate_dataset.py`
3. **Normalizar dados:** `python normalize_data.py`
4. **Treinar modelo:** `python train_model.py`
5. **Prever consumo:** `python predict_consumption.py`

## Configura√ß√£o Avan√ßada

### GitHub Actions

Para configurar o GitHub Actions para criar backups versionados dos commits, crie um arquivo `.github/workflows/backup.yml` com o seguinte conte√∫do:

```yaml
name: Backup Commits

on:
  push:
    branches:
      - main

jobs:
  backup:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Create backups directory
        run: mkdir -p backups

      - name: Zip the code
        run: zip -r backups/$(date +%Y%m%d_%H%M%S).zip .

      - name: Upload artifact
        uses: actions/upload-artifact@v3
        with:
          name: backup
          path: backups
```

### Docker

Crie um arquivo `Dockerfile` com o seguinte conte√∫do:

```dockerfile
FROM python:3.9-slim-buster

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "predict_consumption.py"]
```

### Terraform e Kubernetes

As configura√ß√µes para Terraform e Kubernetes s√£o mais complexas e dependem da sua infraestrutura espec√≠fica.  Voc√™ precisar√° criar arquivos `.tf` e arquivos de configura√ß√£o Kubernetes (deployments, services, etc.) de acordo com suas necessidades.
